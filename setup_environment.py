"""
setup_environment.py - FinGPTé‡åŒ–ç­–ç•¥ç¯å¢ƒé…ç½®è„šæœ¬
è‡ªåŠ¨æ£€æŸ¥å’Œé…ç½®è¿è¡Œç¯å¢ƒ
"""

import os
import sys
import subprocess
import json
from pathlib import Path
from typing import List, Tuple, Optional
import shutil


class EnvironmentSetup:
    """ç¯å¢ƒé…ç½®å™¨"""
    
    def __init__(self):
        self.python_version = sys.version_info
        self.project_root = Path(__file__).parent
        self.errors = []
        self.warnings = []
        self.success_steps = []
        
        print("FinGPTé‡åŒ–ç­–ç•¥ç¯å¢ƒé…ç½®å™¨")
        print("="*50)
    
    def check_python_version(self) -> bool:
        """æ£€æŸ¥Pythonç‰ˆæœ¬"""
        print("\n1. æ£€æŸ¥Pythonç‰ˆæœ¬...")
        
        if self.python_version >= (3, 8):
            print(f"âœ… Pythonç‰ˆæœ¬: {sys.version}")
            self.success_steps.append("Pythonç‰ˆæœ¬æ£€æŸ¥")
            return True
        else:
            error = f"âŒ Pythonç‰ˆæœ¬è¿‡ä½: {sys.version}ï¼Œéœ€è¦Python 3.8+"
            print(error)
            self.errors.append(error)
            return False
    
    def check_gpu_availability(self) -> bool:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        print("\n2. æ£€æŸ¥GPUå¯ç”¨æ€§...")
        
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                
                print(f"âœ… æ£€æµ‹åˆ°GPU: {gpu_name}")
                print(f"âœ… GPUæ˜¾å­˜: {gpu_memory:.1f} GB")
                print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}")
                
                if gpu_memory >= 16:
                    print("âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥è¿è¡ŒFinGPT 13Bæ¨¡å‹")
                    self.success_steps.append("GPUæ£€æŸ¥")
                    return True
                else:
                    warning = f"âš ï¸  æ˜¾å­˜å¯èƒ½ä¸è¶³({gpu_memory:.1f}GB)ï¼Œå»ºè®®16GB+ï¼Œä½†å¯ä»¥å°è¯•8bité‡åŒ–"
                    print(warning)
                    self.warnings.append(warning)
                    self.success_steps.append("GPUæ£€æŸ¥")
                    return True
            else:
                warning = "âš ï¸  æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆæ€§èƒ½æå·®ï¼‰"
                print(warning)
                self.warnings.append(warning)
                return False
                
        except ImportError:
            error = "âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPU"
            print(error)
            self.errors.append(error)
            return False
    
    def check_disk_space(self) -> bool:
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        print("\n3. æ£€æŸ¥ç£ç›˜ç©ºé—´...")
        
        try:
            free_space = shutil.disk_usage('.').free / 1024**3
            required_space = 30  # 30GBç”¨äºæ¨¡å‹ä¸‹è½½å’Œç¼“å­˜
            
            if free_space >= required_space:
                print(f"âœ… å¯ç”¨ç£ç›˜ç©ºé—´: {free_space:.1f} GB")
                self.success_steps.append("ç£ç›˜ç©ºé—´æ£€æŸ¥")
                return True
            else:
                error = f"âŒ ç£ç›˜ç©ºé—´ä¸è¶³: éœ€è¦{required_space}GBï¼Œä»…æœ‰{free_space:.1f}GB"
                print(error)
                self.errors.append(error)
                return False
                
        except Exception as e:
            error = f"âŒ æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}"
            print(error)
            self.errors.append(error)
            return False
    
    def install_dependencies(self) -> bool:
        """å®‰è£…Pythonä¾èµ–"""
        print("\n4. å®‰è£…Pythonä¾èµ–...")
        
        requirements_file = self.project_root / "requirements.txt"
        
        if not requirements_file.exists():
            error = "âŒ requirements.txtæ–‡ä»¶ä¸å­˜åœ¨"
            print(error)
            self.errors.append(error)
            return False
        
        try:
            print("æ­£åœ¨å®‰è£…ä¾èµ–åŒ…ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
            
            # å®‰è£…åŸºç¡€ä¾èµ–
            result = subprocess.run([
                sys.executable, "-m", "pip", "install", "-r", str(requirements_file)
            ], capture_output=True, text=True, timeout=600)
            
            if result.returncode == 0:
                print("âœ… ä¾èµ–åŒ…å®‰è£…æˆåŠŸ")
                self.success_steps.append("ä¾èµ–å®‰è£…")
                return True
            else:
                error = f"âŒ ä¾èµ–å®‰è£…å¤±è´¥: {result.stderr}"
                print(error)
                self.errors.append(error)
                return False
                
        except subprocess.TimeoutExpired:
            error = "âŒ ä¾èµ–å®‰è£…è¶…æ—¶"
            print(error)
            self.errors.append(error)
            return False
        except Exception as e:
            error = f"âŒ ä¾èµ–å®‰è£…å¼‚å¸¸: {e}"
            print(error)
            self.errors.append(error)
            return False
    
    def create_directories(self) -> bool:
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        print("\n5. åˆ›å»ºé¡¹ç›®ç›®å½•...")
        
        directories = [
            'logs',
            'cache', 
            'models',
            'data',
            'output',
            'output/backtest',
            'config'
        ]
        
        try:
            for directory in directories:
                dir_path = self.project_root / directory
                dir_path.mkdir(parents=True, exist_ok=True)
                print(f"âœ… åˆ›å»ºç›®å½•: {directory}")
            
            self.success_steps.append("ç›®å½•åˆ›å»º")
            return True
            
        except Exception as e:
            error = f"âŒ ç›®å½•åˆ›å»ºå¤±è´¥: {e}"
            print(error)
            self.errors.append(error)
            return False
    
    def create_env_template(self) -> bool:
        """åˆ›å»ºç¯å¢ƒå˜é‡æ¨¡æ¿"""
        print("\n6. åˆ›å»ºç¯å¢ƒå˜é‡æ¨¡æ¿...")
        
        env_template = """# FinGPTé‡åŒ–ç­–ç•¥ç¯å¢ƒå˜é‡é…ç½®
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º.envå¹¶å¡«å…¥çœŸå®çš„APIå¯†é’¥

# æ•°æ®æºAPIå¯†é’¥
TIINGO_API_KEY=
FINNHUB_API_KEY=

# äº¤æ˜“æ¥å£ï¼ˆå®ç›˜äº¤æ˜“éœ€è¦ï¼‰
ALPACA_API_KEY_ID=
ALPACA_SECRET_KEY=your_alpaca_secret_key_here

# HuggingFace Tokenï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
HUGGINGFACE_TOKEN=

# AWSé…ç½®ï¼ˆäº‘éƒ¨ç½²éœ€è¦ï¼‰
AWS_ACCESS_KEY_ID=your_aws_access_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
AWS_DEFAULT_REGION=us-east-2

# æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœå·²ä¸‹è½½æ¨¡å‹ï¼‰
LOCAL_FINGPT_MODEL_PATH=/path/to/local/fingpt/model

# è°ƒè¯•è®¾ç½®
DEBUG=False
LOG_LEVEL=INFO
"""
        
        try:
            env_file = self.project_root / ".env.template"
            with open(env_file, 'w', encoding='utf-8') as f:
                f.write(env_template)
            
            print(f"âœ… ç¯å¢ƒå˜é‡æ¨¡æ¿å·²åˆ›å»º: {env_file}")
            print("   è¯·å¤åˆ¶ä¸º.envæ–‡ä»¶å¹¶å¡«å…¥çœŸå®APIå¯†é’¥")
            
            self.success_steps.append("ç¯å¢ƒå˜é‡æ¨¡æ¿")
            return True
            
        except Exception as e:
            error = f"âŒ ç¯å¢ƒå˜é‡æ¨¡æ¿åˆ›å»ºå¤±è´¥: {e}"
            print(error)
            self.errors.append(error)
            return False
    
    def test_model_loading(self) -> bool:
        """æµ‹è¯•æ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼‰"""
        print("\n7. æµ‹è¯•æ¨¡å‹è¿æ¥ï¼ˆå¯é€‰ï¼‰...")
        
        response = input("æ˜¯å¦æµ‹è¯•FinGPTæ¨¡å‹ä¸‹è½½å’ŒåŠ è½½ï¼Ÿ(y/N): ").strip().lower()
        
        if response != 'y':
            print("â­ï¸  è·³è¿‡æ¨¡å‹æµ‹è¯•")
            return True
        
        try:
            print("æ­£åœ¨æµ‹è¯•FinGPTæ¨¡å‹åŠ è½½ï¼Œé¦–æ¬¡è¿è¡Œå°†ä¸‹è½½~27GBæ¨¡å‹æ–‡ä»¶...")
            print("è¿™å¯èƒ½éœ€è¦10-30åˆ†é’Ÿï¼Œå–å†³äºç½‘ç»œé€Ÿåº¦...")
            
            # æµ‹è¯•å¯¼å…¥
            from transformers import LlamaTokenizerFast
            from peft import PeftModel
            
            print("âœ… Transformerså’ŒPEFTåº“å¯ç”¨")
            
            # æµ‹è¯•æ¨¡å‹è·¯å¾„ï¼ˆä¸å®é™…åŠ è½½ï¼‰
            try:
                tokenizer = LlamaTokenizerFast.from_pretrained(
                    "NousResearch/Llama-2-13b-hf",
                    cache_dir="./models"
                )
                print("âœ… åŸºç¡€æ¨¡å‹è¿æ¥æˆåŠŸ")
                self.success_steps.append("æ¨¡å‹è¿æ¥æµ‹è¯•")
                return True
                
            except Exception as e:
                warning = f"âš ï¸  æ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥: {e}"
                print(warning)
                print("   è¿™å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¿è¡Œæ—¶ä¼šè‡ªåŠ¨é‡è¯•")
                self.warnings.append(warning)
                return True
                
        except ImportError as e:
            error = f"âŒ æ¨¡å‹åº“å¯¼å…¥å¤±è´¥: {e}"
            print(error)
            self.errors.append(error)
            return False
        except Exception as e:
            warning = f"âš ï¸  æ¨¡å‹æµ‹è¯•å¼‚å¸¸: {e}"
            print(warning)
            self.warnings.append(warning)
            return True
    
    def create_quick_start_guide(self) -> bool:
        """åˆ›å»ºå¿«é€Ÿå¼€å§‹æŒ‡å—"""
        print("\n8. ç”Ÿæˆå¿«é€Ÿå¼€å§‹æŒ‡å—...")
        
        guide = """# FinGPTé‡åŒ–ç­–ç•¥å¿«é€Ÿå¼€å§‹æŒ‡å—

## 1. ç¯å¢ƒé…ç½®å®Œæˆ âœ…

æ­å–œï¼æ‚¨çš„ç¯å¢ƒå·²é…ç½®å®Œæˆã€‚

## 2. é…ç½®APIå¯†é’¥

1. å¤åˆ¶ `.env.template` ä¸º `.env`
2. å¡«å…¥ä»¥ä¸‹APIå¯†é’¥ï¼š
   - TIINGO_API_KEY: è·å–è‚¡ä»·æ•°æ® (https://api.tiingo.com/)
   - FINNHUB_API_KEY: è·å–æ–°é—»æ•°æ® (https://finnhub.io/)
   - ALPACA_API_KEY: å®ç›˜äº¤æ˜“æ¥å£ (https://alpaca.markets/)

## 3. è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•ç³»ç»Ÿé›†æˆ
python test_fingpt_integration.py

# å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œè¿è¡Œå®Œæ•´å›æµ‹
python run_strategy_backtest.py
```

## 4. ç³»ç»Ÿæ¶æ„

- `factor_model_optimized.py`: FinGPTå› å­æå–
- `signal_generator.py`: äº¤æ˜“ä¿¡å·ç”Ÿæˆ
- `portfolio_manager.py`: æŠ•èµ„ç»„åˆç®¡ç†
- `backtest_engine.py`: å›æµ‹å¼•æ“
- `universe_builder.py`: è‚¡ç¥¨æ± ç®¡ç†

## 5. é…ç½®è°ƒä¼˜

ç¼–è¾‘ `config.py` ä¸­çš„å‚æ•°ï¼š
- ä¿¡å·é˜ˆå€¼
- é£æ§å‚æ•°
- æ¨¡å‹é…ç½®

## 6. AWSéƒ¨ç½²

æ¨èå®ä¾‹ï¼šg5.2xlarge (A10G 24GB)
å‚è€ƒéƒ¨ç½²æŒ‡å—è¿è¡Œã€‚

## é—®é¢˜æ’æŸ¥

- GPUæ˜¾å­˜ä¸è¶³ï¼šå¯ç”¨8bité‡åŒ–
- æ¨¡å‹ä¸‹è½½æ…¢ï¼šä½¿ç”¨æœ¬åœ°ç¼“å­˜æˆ–S3
- APIé™åˆ¶ï¼šæ£€æŸ¥å¯†é’¥å’Œé¢åº¦

ç¥æ‚¨äº¤æ˜“æˆåŠŸï¼ğŸš€
"""
        
        try:
            guide_file = self.project_root / "QUICK_START.md"
            with open(guide_file, 'w', encoding='utf-8') as f:
                f.write(guide)
            
            print(f"âœ… å¿«é€Ÿå¼€å§‹æŒ‡å—å·²åˆ›å»º: {guide_file}")
            self.success_steps.append("å¿«é€Ÿå¼€å§‹æŒ‡å—")
            return True
            
        except Exception as e:
            error = f"âŒ æŒ‡å—åˆ›å»ºå¤±è´¥: {e}"
            print(error)
            self.errors.append(error)
            return False
    
    def run_setup(self) -> bool:
        """è¿è¡Œå®Œæ•´é…ç½®"""
        print("å¼€å§‹FinGPTé‡åŒ–ç­–ç•¥ç¯å¢ƒé…ç½®...\n")
        
        # é…ç½®æ­¥éª¤
        steps = [
            ("Pythonç‰ˆæœ¬æ£€æŸ¥", self.check_python_version),
            ("GPUå¯ç”¨æ€§æ£€æŸ¥", self.check_gpu_availability),
            ("ç£ç›˜ç©ºé—´æ£€æŸ¥", self.check_disk_space),
            ("ä¾èµ–åŒ…å®‰è£…", self.install_dependencies),
            ("ç›®å½•åˆ›å»º", self.create_directories),
            ("ç¯å¢ƒå˜é‡æ¨¡æ¿", self.create_env_template),
            ("æ¨¡å‹è¿æ¥æµ‹è¯•", self.test_model_loading),
            ("å¿«é€Ÿå¼€å§‹æŒ‡å—", self.create_quick_start_guide)
        ]
        
        success_count = 0
        for step_name, step_func in steps:
            try:
                if step_func():
                    success_count += 1
                else:
                    print(f"âš ï¸  æ­¥éª¤å¤±è´¥: {step_name}")
            except KeyboardInterrupt:
                print("\nç”¨æˆ·ä¸­æ–­é…ç½®")
                return False
            except Exception as e:
                print(f"âŒ æ­¥éª¤å¼‚å¸¸: {step_name} - {e}")
                self.errors.append(f"{step_name}å¼‚å¸¸: {e}")
        
        # ç”Ÿæˆé…ç½®æŠ¥å‘Š
        self.generate_setup_report(success_count, len(steps))
        
        return success_count >= len(steps) - 2  # å…è®¸2ä¸ªæ­¥éª¤å¤±è´¥
    
    def generate_setup_report(self, success_count: int, total_steps: int):
        """ç”Ÿæˆé…ç½®æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ç¯å¢ƒé…ç½®æŠ¥å‘Š")
        print("="*60)
        
        print(f"é…ç½®è¿›åº¦: {success_count}/{total_steps}")
        
        if self.success_steps:
            print("\nâœ… æˆåŠŸå®Œæˆ:")
            for step in self.success_steps:
                print(f"   - {step}")
        
        if self.warnings:
            print("\nâš ï¸  è­¦å‘Š:")
            for warning in self.warnings:
                print(f"   - {warning}")
        
        if self.errors:
            print("\nâŒ é”™è¯¯:")
            for error in self.errors:
                print(f"   - {error}")
        
        if success_count >= total_steps - 2:
            print("\nğŸ‰ ç¯å¢ƒé…ç½®åŸºæœ¬å®Œæˆï¼")
            print("\nä¸‹ä¸€æ­¥:")
            print("1. å¤åˆ¶ .env.template ä¸º .env å¹¶å¡«å…¥APIå¯†é’¥")
            print("2. è¿è¡Œé›†æˆæµ‹è¯•: python test_fingpt_integration.py")
            print("3. å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œè¿è¡Œç­–ç•¥: python run_strategy_backtest.py")
        else:
            print("\nâš ï¸  ç¯å¢ƒé…ç½®æœªå®Œå…¨æˆåŠŸï¼Œè¯·ä¿®å¤é”™è¯¯åé‡è¯•")
        
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    try:
        setup = EnvironmentSetup()
        success = setup.run_setup()
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­é…ç½®")
        return 1
    except Exception as e:
        print(f"\n\né…ç½®è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

